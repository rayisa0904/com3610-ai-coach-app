# COM3610 Dissertation Project
# AI Driving Coach on Edge
# Authors: Weixiang Han (Ray)
# Date: 2025-05-14

import fastf1
import base64
import pandas as pd
import requests

class AICoach:
    """
    A class to provide AI-driven coaching for drivers by comparing player telemetry data
    with reference data, and generates actionable insights.
    """
    def __init__(self):
        """
        Initialise the AI Coach with reference data and model endpoint configuration.
        """

        # Silverstone 2024 Qualifying fastest lap by George Russell
        self.reference_driver = "RUS"  # George Russell
        self.year = 2024
        self.event_name = "Silverstone"
        self.session_type = "Q"  # Qualifying
        self.model_endpoint = "http://172.20.10.2:1234/v1/chat/completions" # Replace with your LM Studio endpoint

        # Load reference telemetry data for comparison
        self.load_reference_data()

    def load_reference_data(self):
        """
        Load reference telemetry data for the specified driver, event, and session type.
        This data will be used for lap comparisons.
        """
        session = fastf1.get_session(self.year, self.event_name, self.session_type)
        session.load()
        self.reference_lap = session.laps.pick_drivers(self.reference_driver).pick_fastest()
        self.reference_telemetry = self.reference_lap.get_car_data().add_distance()

    def compare_laps(self, player_lap_telemetry: pd.DataFrame, lap_invalid_but_completed: bool = False) -> str:
        """
        Compare the player's lap telemetry data with the reference lap telemetry data
        and generate a coaching tip using an AI model.

        Args:
            player_lap_telemetry (pd.DataFrame): Telemetry data for the player's lap.
            lap_invalid_but_completed (bool): Whether the lap was invalid but completed.

        Returns:
            str: A coaching tip generated by the AI model.
        """
        # Check if telemetry data is valid
        if not isinstance(player_lap_telemetry, pd.DataFrame):
            return "Telemetry data not ready yet."

        if player_lap_telemetry.empty:
            return "No player telemetry data available to compare."

        # Copy reference and player telemetry data
        ref = self.reference_telemetry.copy()
        player = player_lap_telemetry.copy()

        # Ensure required telemetry fields are present
        required_columns = ['Speed', 'Throttle', 'Brake']
        missing_columns = [col for col in required_columns if col not in player.columns]
        if missing_columns:
            return f"Missing telemetry fields: {', '.join(missing_columns)}."

        # Calculate averages for speed, throttle, and brake
        ref_avg_speed = ref['Speed'].mean()
        ref_avg_throttle = ref['Throttle'].mean()
        ref_avg_brake = ref['Brake'].mean()

        player_avg_speed = player['Speed'].mean()
        player_avg_throttle = player['Throttle'].mean()
        player_avg_brake = player['Brake'].mean()
        
        # Compute deltas between reference and player telemetry
        delta_speed = ref_avg_speed - player_avg_speed
        delta_throttle = ref_avg_throttle - player_avg_throttle
        delta_brake = ref_avg_brake - player_avg_brake

        # Generate a basic analysis summary
        analysis = f"""
        Comparing your lap to George Russell's fastest qualifying lap at Silverstone 2024:
        - Your average speed is {delta_speed:.1f} km/h lower.
        - Your throttle usage is {delta_throttle:.1f} different.
        - Your braking is {delta_brake:.1f} different.
        """

        print("[DEBUG] Quick Analysis for LLM:")
        print(analysis)

        # Query the AI model for a coaching tip
        return self.query_llm(analysis, lap_invalid_but_completed=lap_invalid_but_completed)

    def query_llm(self, analysis_text: str, lap_invalid_but_completed: bool = False) -> str:
        """
        Query the AI model to generate a coaching tip based on the analysis text.

        Args:
            analysis_text (str): The analysis summary to provide to the AI model.
            lap_invalid_but_completed (bool): Whether the lap was invalid but completed.

        Returns:
            str: A coaching tip generated by the AI model.
        """
        headers = {
            "Content-Type": "application/json"
        }

        # Define the system prompt based on lap validity
        if lap_invalid_but_completed:
            system_prompt = (
                "You are a friendly F1 coach. "
                "The driver exceeded track limits but completed the lap. "
                "Give one short positive feedback and one short advice for improvement. No long explanations."
            )
        else:
            system_prompt = (
                "You are a professional F1 engineer coach. "
                "Give the driver a direct, actionable tip based on telemetry. "
                "Respond in 1-2 short sentences maximum. Focus only on improvements."
            )

        body = {
            "model": "gemma-3-12b-it",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": analysis_text}
            ],
            "temperature": 0.2,
            "max_tokens": 200
        }

        try:
            response = requests.post(self.model_endpoint, json=body, headers=headers, timeout=200)
            response.raise_for_status()
            data = response.json()
            return data['choices'][0]['message']['content']
        
        except Exception as e:
            print(f"⚠️ Error querying LM Studio: {e}")
            return "Sorry, I could not generate a coaching tip right now."

    def analyze_graph_image(self, image_path: str) -> str:
        """
        Analyse telemetry graph image and generate insights using the AI model.

        Args:
            image_path (str): The path to the telemetry graph image.

        Returns:
            str: Insights generated by the AI model based on the graph.
        """
        try:
            # Read and encode the image as base64
            with open(image_path, "rb") as img_file:
                img_data = base64.b64encode(img_file.read()).decode("utf-8")
            
            data_uri = f"data:image/png;base64,{img_data}"

            headers = {
                "Content-Type": "application/json"
            }

            body = {
                "model": "gemma-3-12b-it",
                "messages": [
                    {"role": "system", "content": (
                        "You are a professional F1 race engineer. "
                        "Give the driver a direct, actionable tip based this telemetry graph from Silverstone."
                        "Respond in 1-2 short sentences maximum for individual graphs. Focus only on improvements."
                    )},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Please analyse this telemetry graph and summarise key areas for improvement."},
                            {"type": "image_url", "image_url": {"url": data_uri}}
                        ]
                    }
                ],
                "temperature": 0.2,
                "max_tokens": 600
            }

            print("[DEBUG] Sending request to LM Studio...")
            response = requests.post(self.model_endpoint, json=body, headers=headers, timeout=500)

            print(f"[DEBUG] HTTP Status: {response.status_code}")
            print("[DEBUG] Raw response text:")
            print(response.text)

            response.raise_for_status()  # will raise HTTPError if status != 200

            # Parse the response
            data = response.json()
            print("[DEBUG] Parsed JSON response:", data)

            # Check for choices field
            if not data.get("choices"):
                print("⚠️ 'choices' field missing from response JSON.")
                return "AI insights not available: 'choices' missing in LLM response."

            message_obj = data["choices"][0].get("message")
            if not message_obj:
                print("⚠️ 'message' object missing in first choice.")
                return "AI insights not available: 'message' missing in LLM response."

            content = message_obj.get("content")
            if not content:
                print("⚠️ 'content' missing inside 'message'.")
                return "AI insights not available: 'content' missing in LLM response."

            print("[DEBUG] Final extracted content:", content)
            return content

        except KeyError as e:
            print(f"⚠️ KeyError parsing LLM response: {e}")
            return "AI insights parsing error (KeyError)."

        except requests.HTTPError as e:
            print(f"⚠️ HTTP error from LM Studio: {e}")
            return f"AI insights not available: HTTP error {e.response.status_code}."

        except Exception as e:
            print(f"⚠️ General exception querying LM Studio: {e}")
            return "Unable to generate AI insights from the graph at this time."
